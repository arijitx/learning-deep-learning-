{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math,sys\n",
    "\n",
    "data=pd.read_csv('dataset/data.csv')\n",
    "def one_zero(x):\n",
    "    if x==-1:\n",
    "        return 0\n",
    "    return x\n",
    "data['label']=data['label'].apply(one_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://gist.github.com/rougier/c0d31f5cbdaac27b876c\n",
    "def progress(value,  length=40, title = \" \", vmin=0.0, vmax=1.0):\n",
    "    # Block progression is 1/8\n",
    "    blocks = [\"\", \"▏\",\"▎\",\"▍\",\"▌\",\"▋\",\"▊\",\"▉\",\"█\"]\n",
    "    vmin = vmin or 0.0\n",
    "    vmax = vmax or 1.0\n",
    "    lsep, rsep = \"▏\", \"▕\"\n",
    "    value = min(max(value, vmin), vmax)\n",
    "    value = (value-vmin)/float(vmax-vmin)\n",
    "    v = value*length\n",
    "    x = math.floor(v) # integer part\n",
    "    y = v - x         # fractional part\n",
    "    base = 0.125      # 0.125 = 1/8\n",
    "    prec = 3\n",
    "    i = int(round(base*math.floor(float(y)/base),prec)/base)\n",
    "    bar = \"█\"*x + blocks[i]\n",
    "    n = length-len(bar)\n",
    "    bar = lsep + bar + \" \"*n + rsep\n",
    "    sys.stdout.write(\"\\r\" + title + bar + \" %.1f%%\" % (value*100))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "class neural_net():\n",
    "    def __init__(self,ni,nh,no,lr=0.01,max_steps=10):\n",
    "        self._lr=lr\n",
    "        self._max_steps=max_steps\n",
    "        self._n_hidden=nh\n",
    "        self._n_output=no\n",
    "        self._n_input=ni\n",
    "        self._layers=[]\n",
    "        self._labels=[]\n",
    "        last_len=self._n_input+1\n",
    "        idx=1\n",
    "        for e in self._n_hidden:\n",
    "            hidden_layer = [{'weights':np.random.rand(last_len),'name':'hidden_layer_'+str(idx)+'_unit_'+str(i+1)} for i in range(e)]\n",
    "            self._layers.append(hidden_layer)\n",
    "            last_len=e\n",
    "            idx+=1\n",
    "        output_layer= [{'weights':np.random.rand(last_len ),'name':'output_layer_unit_'+str(i+1)} for i in range(self._n_output)]    \n",
    "        self._layers.append(output_layer)\n",
    "    \n",
    "    def preprocessing(self,X,Y):\n",
    "        X=np.hstack((X,np.ones(X.shape[0]).reshape((X.shape[0],1))))\n",
    "        if Y is not None:\n",
    "            self._labels=list(set(Y.flatten()))\n",
    "            labels=dict(enumerate(self._labels))\n",
    "            labels={v: k for k, v in labels.items()}\n",
    "            new_y=np.zeros((Y.shape[0],len(labels)))\n",
    "            for i in range(Y.shape[0]):\n",
    "                new_y[i][labels[Y[i][0]]]=1\n",
    "            Y=new_y\n",
    "        return X,Y\n",
    "    \n",
    "    def activation(self,x,name='sigmoid'):\n",
    "        if name=='sigmoid':\n",
    "            return 1./(1.+np.exp(-x))\n",
    "        else:\n",
    "            return 1.\n",
    "    \n",
    "    def output(self,inputs,weights):\n",
    "        return np.dot(weights.T,inputs)\n",
    "    \n",
    "    def forward_prop(self,row):\n",
    "        inputs=row\n",
    "        for layer in self._layers:\n",
    "            new_inputs=[]\n",
    "            for unit in layer:\n",
    "                x=self.output(inputs,unit['weights'])\n",
    "                unit['output']=self.activation(x)\n",
    "                new_inputs.append(unit['output'])\n",
    "            inputs=np.array(new_inputs)\n",
    "        return inputs\n",
    "    \n",
    "    def derivate_b(self,x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    def back_prop_error(self,y):\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer=self._layers[i]\n",
    "            if i==len(self._layers)-1:\n",
    "                for j in range(len(layer)):\n",
    "                    layer[j]['delta']=y[j]-layer[j]['output']\n",
    "            else:   \n",
    "                for j in range(len(layer)):\n",
    "                    error=0.\n",
    "                    for unit in self._layers[i+1]:\n",
    "                        error+=unit['weights'][j]*unit['delta']\n",
    "                    layer[j]['delta']=error*self.derivate_b(layer[j]['output'])\n",
    "    \n",
    "    def update_weights(self,row):\n",
    "        for i in range(len(self._layers)):\n",
    "            layer=self._layers[i]\n",
    "            inputs=row\n",
    "            #for other layer except the first input is output of previous node\n",
    "            if(i!=0):\n",
    "                inputs=[unit['output'] for unit in self._layers[i-1]]\n",
    "            for unit in self._layers[i]:\n",
    "                unit['weights']+=unit['delta']*self._lr*np.array(inputs)\n",
    "            \n",
    "    def loss(self,X,Y):\n",
    "        #cross entropy loss\n",
    "        m=X.shape[0]\n",
    "        error=0.\n",
    "        for i in range(X.shape[0]):\n",
    "            output=self.forward_prop(X[i])\n",
    "            for j in range(Y.shape[1]):\n",
    "                error+=Y[i][j]*np.log(output[j])+(1-Y[i][j])*np.log(1-output[j])\n",
    "        return -error/m\n",
    "    \n",
    "    def train(self,X,Y):\n",
    "        X,Y=self.preprocessing(X,Y)\n",
    "        \n",
    "        for step in range(self._max_steps):\n",
    "            percentage=1\n",
    "            for i in range(X.shape[0]):\n",
    "                self.forward_prop(X[i])\n",
    "                self.back_prop_error(Y[i])\n",
    "                self.update_weights(X[i])\n",
    "                if((i/X.shape[0])*100>percentage-1):\n",
    "                    progress(i/X.shape[0]+.01,title='Epoch : '+str(step+1))\n",
    "                    percentage+=1\n",
    "            print('\\nEpoch:',step+1,'Loss:',self.loss(X,Y))\n",
    "                \n",
    "                    \n",
    "    def predict(self,X):\n",
    "        X,_=self.preprocessing(X,None)\n",
    "        res_full=[]\n",
    "        for i in range(X.shape[0]):\n",
    "            res=[]\n",
    "            self.forward_prop(X[i])\n",
    "            for unit in self._layers[-1]:\n",
    "                res.append(unit['output'])\n",
    "            res_full.append(self._labels[np.argmax(np.array(res))])\n",
    "            #res_full.append(res)\n",
    "        return np.array(res_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 1 Loss: 1.85685129062\n",
      "Epoch : 2▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 2 Loss: 1.88902351545\n",
      "Epoch : 3▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 3 Loss: 1.872657279\n",
      "Epoch : 4▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 4 Loss: 1.8530375063\n",
      "Epoch : 5▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 5 Loss: 1.83335052907\n",
      "Epoch : 6▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 6 Loss: 1.81320306288\n",
      "Epoch : 7▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 7 Loss: 1.79117586455\n",
      "Epoch : 8▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 8 Loss: 1.7651999918\n",
      "Epoch : 9▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 9 Loss: 1.73242523995\n",
      "Epoch : 10▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 10 Loss: 1.69428898819\n",
      "Epoch : 11▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 11 Loss: 1.65908035408\n",
      "Epoch : 12▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 12 Loss: 1.62876947878\n",
      "Epoch : 13▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 13 Loss: 1.60904371783\n",
      "Epoch : 14▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 14 Loss: 1.65230481203\n",
      "Epoch : 15▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 15 Loss: 1.70693783378\n",
      "Epoch : 16▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 16 Loss: 1.77809617875\n",
      "Epoch : 17▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 17 Loss: 1.75808548617\n",
      "Epoch : 18▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 18 Loss: 1.53901056116\n",
      "Epoch : 19▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 19 Loss: 1.1531068313\n",
      "Epoch : 20▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 20 Loss: 2.33022241059\n",
      "Epoch : 21▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 21 Loss: 2.1742940648\n",
      "Epoch : 22▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 22 Loss: 1.75447800166\n",
      "Epoch : 23▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 23 Loss: 0.28155747023\n",
      "Epoch : 24▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 24 Loss: 0.618078972651\n",
      "Epoch : 25▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 25 Loss: 0.192366363865\n",
      "Epoch : 26▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 26 Loss: 0.587879471366\n",
      "Epoch : 27▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 27 Loss: 0.862986565704\n",
      "Epoch : 28▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 28 Loss: 2.85736274116\n",
      "Epoch : 29▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 29 Loss: 0.59444430716\n",
      "Epoch : 30▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 30 Loss: 1.46594207374\n",
      "Epoch : 31▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 31 Loss: 2.89677069314\n",
      "Epoch : 32▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 32 Loss: 2.71936491444\n",
      "Epoch : 33▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 33 Loss: 1.532905264\n",
      "Epoch : 34▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 34 Loss: 0.301287613243\n",
      "Epoch : 35▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 35 Loss: 0.155600872742\n",
      "Epoch : 36▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 36 Loss: 0.109986416027\n",
      "Epoch : 37▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 37 Loss: 2.19071283101\n",
      "Epoch : 38▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 38 Loss: 1.87423690916\n",
      "Epoch : 39▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 39 Loss: 2.35773006873\n",
      "Epoch : 40▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 40 Loss: 0.604164677258\n",
      "Epoch : 41▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 41 Loss: 0.253646102076\n",
      "Epoch : 42▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 42 Loss: 0.0773051909411\n",
      "Epoch : 43▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 43 Loss: 0.0894037889627\n",
      "Epoch : 44▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 44 Loss: 0.040485002233\n",
      "Epoch : 45▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 45 Loss: 0.0363999076203\n",
      "Epoch : 46▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 46 Loss: 0.0329192171073\n",
      "Epoch : 47▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 47 Loss: 0.0301419774542\n",
      "Epoch : 48▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 48 Loss: 0.0279016546811\n",
      "Epoch : 49▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 49 Loss: 0.0260193921313\n",
      "Epoch : 50▏████████████████████████████████████████▕ 100.0%\n",
      "Epoch: 50 Loss: 0.0244009013483\n"
     ]
    }
   ],
   "source": [
    "X=data[['x','y']].as_matrix()\n",
    "Y=data[['label']].as_matrix()\n",
    "\n",
    "nn=neural_net(2,[3,2],2,lr=0.1,max_steps=50)\n",
    "nn.train(X,Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
