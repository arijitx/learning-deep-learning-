{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Variational Auto Encoders </h1>\n",
    "\n",
    "<h2> AutoEncoders </h2>\n",
    "\n",
    "Autoencoders are special type of encoder decoder neural network where on one side you feed your data and on the other side you want to reconstruct the same data , in the middle you try to compress the data to a vector **z** where **len(z) << len(data)** . A typical autoencoder looks like this \n",
    "\n",
    "![autoencoder](https://wiki.tum.de/download/attachments/25007151/autoencoder_structure.png?version=2&modificationDate=1485704320100&api=v2)\n",
    "![autoencoder](https://cdn-images-1.medium.com/max/703/0*yGqTBMopqHbR0fcF.)\n",
    "\n",
    "This autoencoders are good for compression of data , but not in the case when we cant to generate new data from our training images or generate data which are completely new and never seen in the training examples , but somewhere related to our training images.\n",
    "\n",
    "More formally we can define it as we want to build a generative model , which on trained on samples can learn and produce new samples which are from the training samples distribution, so our goal is to not only learn a netowork which can compress the training images and reconstruct it back but also which can learn the distribution which the training samples follow.\n",
    "\n",
    "<h2> Variational Autoencoders </h2>\n",
    "\n",
    "So in a variational autoencoder our task to is find out the distribution of our training samples and furthur produce samples from that distribution.\n",
    "\n",
    "![image](https://camo.githubusercontent.com/74620840800d49e0e3f0fb97db950212f61ec596/687474703a2f2f6b766672616e732e636f6d2f636f6e74656e742f696d616765732f323031362f30382f7661652e6a7067)\n",
    "\n",
    "Here we need to minimize two losses , One the **log liklihood loss** and the other is the **KL Divergence** , The first will ensure that we are able to reconstruct our training samples and the second ensures that we can find out the distribution of the training sample.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Here we use a CNN as an Encoder with layers and relu as activation and in Decoder we use Billinear Upsample. All the images are resized to 150x150.\n",
    "\n",
    "\n",
    "# Results\n",
    "\n",
    "## Reconstructions\n",
    "\n",
    "#### Epoch 28\n",
    "![recon](results/reconstruction_28.png)\n",
    "#### Epoch 20\n",
    "![recon](results/reconstruction_20.png)\n",
    "#### Epoch 10\n",
    "![recon](results/reconstruction_10.png)\n",
    "#### Epoch 2\n",
    "![recon](results/reconstruction_2.png)\n",
    "\n",
    "## Generations\n",
    "\n",
    "#### Epoch 28\n",
    "![gen](results/sample_28.png)\n",
    "#### Epoch 20\n",
    "![gen](results/sample_20.png)\n",
    "#### Epoch 10\n",
    "![gen](results/sample_10.png)\n",
    "#### Epoch 2\n",
    "![gen](results/sample_2.png)\n",
    "\n",
    "## Generation Epoch 1 to Epoch 28\n",
    "\n",
    "![gen](results/generation.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "log_interval=10\n",
    "CUDA_DEVICE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('data/train_celebA',\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.CenterCrop(150),\n",
    "                       transforms.ToTensor()\n",
    "                   ])),\n",
    "                    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('data/test_celebA',\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.CenterCrop(150),\n",
    "                       transforms.ToTensor()\n",
    "                   ])),\n",
    "                    batch_size=batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "        self.cnv1=nn.Conv2d(3,10,3,2)\n",
    "        self.cnv2=nn.Conv2d(10,3,4,2)\n",
    "        self.fc1=nn.Linear(3*36*36,50)\n",
    "        self.fc2=nn.Linear(3*36*36,50)\n",
    "        self.fc3=nn.Linear(50,3*36*36)\n",
    "        self.up2=nn.Upsample(scale_factor=2,mode='bilinear')\n",
    "        self.pd2=nn.ReplicationPad2d(2)\n",
    "        self.uc2=nn.Conv2d(3,10,3,1)\n",
    "        self.up1=nn.Upsample(scale_factor=2,mode='bilinear')\n",
    "        self.pd1=nn.ReplicationPad2d(2)\n",
    "        self.uc1=nn.Conv2d(10,3,3,1)\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def encode(self,x):\n",
    "        h=self.relu(self.cnv1(x))\n",
    "        h=self.relu(self.cnv2(h))\n",
    "        h=h.view(-1,3*36*36)\n",
    "        return self.fc1(h),self.fc2(h)\n",
    "    \n",
    "    def reparameterize(self,mean,logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mean)\n",
    "        else:\n",
    "            return mean\n",
    "    \n",
    "    def decode(self,x):\n",
    "        x=self.fc3(x)\n",
    "        x=x.view(-1,3,36,36)\n",
    "        h=self.uc2(self.pd2(self.up2(x)))\n",
    "        h=self.uc1(self.pd1(self.up1(h)))\n",
    "        return self.sigmoid(h)\n",
    "        \n",
    "\n",
    "    def forward(self,im):\n",
    "        mu,logvar=self.encode(im)\n",
    "        z=self.reparameterize(mu,logvar)\n",
    "        return self.decode(z),mu,logvar\n",
    "    \n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = F.mse_loss(recon_x,x,size_average=False)\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data).cuda(CUDA_DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        data = data.cuda(CUDA_DEVICE)\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            comparison = torch.cat([data[:n],recon_batch.view(batch_size, 3, 150, 150)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                     'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 5615.964355\n",
      "Train Epoch: 1 [2560/30000 (8%)]\tLoss: 5730.507324\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 5032.255371\n",
      "Train Epoch: 1 [7680/30000 (25%)]\tLoss: 4835.468750\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 4270.423828\n",
      "Train Epoch: 1 [12800/30000 (42%)]\tLoss: 3731.114990\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 3333.924316\n",
      "Train Epoch: 1 [17920/30000 (59%)]\tLoss: 3036.560303\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 2808.180420\n",
      "Train Epoch: 1 [23040/30000 (76%)]\tLoss: 2395.952881\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 2100.758057\n",
      "Train Epoch: 1 [28160/30000 (93%)]\tLoss: 2075.077393\n",
      "====> Epoch: 1 Average loss: 3644.7103\n",
      "====> Test set loss: 1987.1814\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 2059.728027\n",
      "Train Epoch: 2 [2560/30000 (8%)]\tLoss: 2022.618652\n",
      "Train Epoch: 2 [5120/30000 (17%)]\tLoss: 1906.207520\n",
      "Train Epoch: 2 [7680/30000 (25%)]\tLoss: 1842.277344\n",
      "Train Epoch: 2 [10240/30000 (34%)]\tLoss: 1764.486694\n",
      "Train Epoch: 2 [12800/30000 (42%)]\tLoss: 1766.281372\n",
      "Train Epoch: 2 [15360/30000 (51%)]\tLoss: 1757.711060\n",
      "Train Epoch: 2 [17920/30000 (59%)]\tLoss: 1723.738770\n",
      "Train Epoch: 2 [20480/30000 (68%)]\tLoss: 1676.748291\n",
      "Train Epoch: 2 [23040/30000 (76%)]\tLoss: 1610.205566\n",
      "Train Epoch: 2 [25600/30000 (85%)]\tLoss: 1558.202759\n",
      "Train Epoch: 2 [28160/30000 (93%)]\tLoss: 1589.545166\n",
      "====> Epoch: 2 Average loss: 1767.4049\n",
      "====> Test set loss: 1518.6479\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 1584.056030\n",
      "Train Epoch: 3 [2560/30000 (8%)]\tLoss: 1555.906982\n",
      "Train Epoch: 3 [5120/30000 (17%)]\tLoss: 1536.995361\n",
      "Train Epoch: 3 [7680/30000 (25%)]\tLoss: 1500.849365\n",
      "Train Epoch: 3 [10240/30000 (34%)]\tLoss: 1393.354858\n",
      "Train Epoch: 3 [12800/30000 (42%)]\tLoss: 1469.187622\n",
      "Train Epoch: 3 [15360/30000 (51%)]\tLoss: 1409.048584\n",
      "Train Epoch: 3 [17920/30000 (59%)]\tLoss: 1390.599243\n",
      "Train Epoch: 3 [20480/30000 (68%)]\tLoss: 1356.104248\n",
      "Train Epoch: 3 [23040/30000 (76%)]\tLoss: 1319.169556\n",
      "Train Epoch: 3 [25600/30000 (85%)]\tLoss: 1356.407593\n",
      "Train Epoch: 3 [28160/30000 (93%)]\tLoss: 1385.049805\n",
      "====> Epoch: 3 Average loss: 1438.5113\n",
      "====> Test set loss: 1305.9612\n",
      "Train Epoch: 4 [0/30000 (0%)]\tLoss: 1397.990845\n",
      "Train Epoch: 4 [2560/30000 (8%)]\tLoss: 1306.918213\n",
      "Train Epoch: 4 [5120/30000 (17%)]\tLoss: 1315.959961\n",
      "Train Epoch: 4 [7680/30000 (25%)]\tLoss: 1326.183716\n",
      "Train Epoch: 4 [10240/30000 (34%)]\tLoss: 1301.141846\n",
      "Train Epoch: 4 [12800/30000 (42%)]\tLoss: 1279.052368\n",
      "Train Epoch: 4 [15360/30000 (51%)]\tLoss: 1238.015381\n",
      "Train Epoch: 4 [17920/30000 (59%)]\tLoss: 1309.333740\n",
      "Train Epoch: 4 [20480/30000 (68%)]\tLoss: 1252.708008\n",
      "Train Epoch: 4 [23040/30000 (76%)]\tLoss: 1196.471680\n",
      "Train Epoch: 4 [25600/30000 (85%)]\tLoss: 1313.125732\n",
      "Train Epoch: 4 [28160/30000 (93%)]\tLoss: 1306.700928\n",
      "====> Epoch: 4 Average loss: 1286.9279\n",
      "====> Test set loss: 1247.9364\n",
      "Train Epoch: 5 [0/30000 (0%)]\tLoss: 1302.222290\n",
      "Train Epoch: 5 [2560/30000 (8%)]\tLoss: 1277.605713\n",
      "Train Epoch: 5 [5120/30000 (17%)]\tLoss: 1223.262329\n",
      "Train Epoch: 5 [7680/30000 (25%)]\tLoss: 1176.665771\n",
      "Train Epoch: 5 [10240/30000 (34%)]\tLoss: 1213.919434\n",
      "Train Epoch: 5 [12800/30000 (42%)]\tLoss: 1224.573486\n",
      "Train Epoch: 5 [15360/30000 (51%)]\tLoss: 1156.189941\n",
      "Train Epoch: 5 [17920/30000 (59%)]\tLoss: 1134.934814\n",
      "Train Epoch: 5 [20480/30000 (68%)]\tLoss: 1182.958374\n",
      "Train Epoch: 5 [23040/30000 (76%)]\tLoss: 1147.208740\n",
      "Train Epoch: 5 [25600/30000 (85%)]\tLoss: 1142.339844\n",
      "Train Epoch: 5 [28160/30000 (93%)]\tLoss: 1113.001953\n",
      "====> Epoch: 5 Average loss: 1185.7599\n",
      "====> Test set loss: 1093.3260\n",
      "Train Epoch: 6 [0/30000 (0%)]\tLoss: 1169.190918\n",
      "Train Epoch: 6 [2560/30000 (8%)]\tLoss: 1149.827148\n",
      "Train Epoch: 6 [5120/30000 (17%)]\tLoss: 1100.410400\n",
      "Train Epoch: 6 [7680/30000 (25%)]\tLoss: 1152.354370\n",
      "Train Epoch: 6 [10240/30000 (34%)]\tLoss: 1102.220215\n",
      "Train Epoch: 6 [12800/30000 (42%)]\tLoss: 1125.967041\n",
      "Train Epoch: 6 [15360/30000 (51%)]\tLoss: 1099.321045\n",
      "Train Epoch: 6 [17920/30000 (59%)]\tLoss: 1115.361084\n",
      "Train Epoch: 6 [20480/30000 (68%)]\tLoss: 1106.227295\n",
      "Train Epoch: 6 [23040/30000 (76%)]\tLoss: 1096.734253\n",
      "Train Epoch: 6 [25600/30000 (85%)]\tLoss: 1075.643555\n",
      "Train Epoch: 6 [28160/30000 (93%)]\tLoss: 1002.620300\n",
      "====> Epoch: 6 Average loss: 1097.5731\n",
      "====> Test set loss: 1025.9389\n",
      "Train Epoch: 7 [0/30000 (0%)]\tLoss: 1083.610107\n",
      "Train Epoch: 7 [2560/30000 (8%)]\tLoss: 1092.817017\n",
      "Train Epoch: 7 [5120/30000 (17%)]\tLoss: 1054.791138\n",
      "Train Epoch: 7 [7680/30000 (25%)]\tLoss: 1063.647949\n",
      "Train Epoch: 7 [10240/30000 (34%)]\tLoss: 1073.180664\n",
      "Train Epoch: 7 [12800/30000 (42%)]\tLoss: 1070.499268\n",
      "Train Epoch: 7 [15360/30000 (51%)]\tLoss: 1052.967285\n",
      "Train Epoch: 7 [17920/30000 (59%)]\tLoss: 1062.961060\n",
      "Train Epoch: 7 [20480/30000 (68%)]\tLoss: 1081.458130\n",
      "Train Epoch: 7 [23040/30000 (76%)]\tLoss: 1030.054199\n",
      "Train Epoch: 7 [25600/30000 (85%)]\tLoss: 1025.821411\n",
      "Train Epoch: 7 [28160/30000 (93%)]\tLoss: 1036.209839\n",
      "====> Epoch: 7 Average loss: 1044.8824\n",
      "====> Test set loss: 996.6736\n",
      "Train Epoch: 8 [0/30000 (0%)]\tLoss: 993.820618\n",
      "Train Epoch: 8 [2560/30000 (8%)]\tLoss: 1031.770996\n",
      "Train Epoch: 8 [5120/30000 (17%)]\tLoss: 1000.926514\n",
      "Train Epoch: 8 [7680/30000 (25%)]\tLoss: 1048.386841\n",
      "Train Epoch: 8 [10240/30000 (34%)]\tLoss: 1001.516479\n",
      "Train Epoch: 8 [12800/30000 (42%)]\tLoss: 1030.958984\n",
      "Train Epoch: 8 [15360/30000 (51%)]\tLoss: 978.979980\n",
      "Train Epoch: 8 [17920/30000 (59%)]\tLoss: 1005.478271\n",
      "Train Epoch: 8 [20480/30000 (68%)]\tLoss: 1008.693115\n",
      "Train Epoch: 8 [23040/30000 (76%)]\tLoss: 1006.300537\n",
      "Train Epoch: 8 [25600/30000 (85%)]\tLoss: 1000.829773\n",
      "Train Epoch: 8 [28160/30000 (93%)]\tLoss: 1031.668823\n",
      "====> Epoch: 8 Average loss: 1017.8773\n",
      "====> Test set loss: 1007.9752\n",
      "Train Epoch: 9 [0/30000 (0%)]\tLoss: 1052.187500\n",
      "Train Epoch: 9 [2560/30000 (8%)]\tLoss: 1058.222168\n",
      "Train Epoch: 9 [5120/30000 (17%)]\tLoss: 1040.342896\n",
      "Train Epoch: 9 [7680/30000 (25%)]\tLoss: 974.244934\n",
      "Train Epoch: 9 [10240/30000 (34%)]\tLoss: 1010.886597\n",
      "Train Epoch: 9 [12800/30000 (42%)]\tLoss: 980.305237\n",
      "Train Epoch: 9 [15360/30000 (51%)]\tLoss: 990.459473\n",
      "Train Epoch: 9 [17920/30000 (59%)]\tLoss: 1016.564514\n",
      "Train Epoch: 9 [20480/30000 (68%)]\tLoss: 959.138489\n",
      "Train Epoch: 9 [23040/30000 (76%)]\tLoss: 962.268677\n",
      "Train Epoch: 9 [25600/30000 (85%)]\tLoss: 993.574585\n",
      "Train Epoch: 9 [28160/30000 (93%)]\tLoss: 914.031738\n",
      "====> Epoch: 9 Average loss: 1000.0324\n",
      "====> Test set loss: 951.4806\n",
      "Train Epoch: 10 [0/30000 (0%)]\tLoss: 990.782166\n",
      "Train Epoch: 10 [2560/30000 (8%)]\tLoss: 999.896423\n",
      "Train Epoch: 10 [5120/30000 (17%)]\tLoss: 957.497864\n",
      "Train Epoch: 10 [7680/30000 (25%)]\tLoss: 987.849915\n",
      "Train Epoch: 10 [10240/30000 (34%)]\tLoss: 1000.064453\n",
      "Train Epoch: 10 [12800/30000 (42%)]\tLoss: 968.058289\n",
      "Train Epoch: 10 [15360/30000 (51%)]\tLoss: 920.810120\n",
      "Train Epoch: 10 [17920/30000 (59%)]\tLoss: 965.552734\n",
      "Train Epoch: 10 [20480/30000 (68%)]\tLoss: 954.748291\n",
      "Train Epoch: 10 [23040/30000 (76%)]\tLoss: 1017.584167\n",
      "Train Epoch: 10 [25600/30000 (85%)]\tLoss: 965.791504\n",
      "Train Epoch: 10 [28160/30000 (93%)]\tLoss: 944.758057\n",
      "====> Epoch: 10 Average loss: 982.6039\n",
      "====> Test set loss: 1066.4519\n",
      "Train Epoch: 11 [0/30000 (0%)]\tLoss: 1125.781372\n",
      "Train Epoch: 11 [2560/30000 (8%)]\tLoss: 1034.224976\n",
      "Train Epoch: 11 [5120/30000 (17%)]\tLoss: 976.141479\n",
      "Train Epoch: 11 [7680/30000 (25%)]\tLoss: 995.104553\n",
      "Train Epoch: 11 [10240/30000 (34%)]\tLoss: 952.194824\n",
      "Train Epoch: 11 [12800/30000 (42%)]\tLoss: 955.452576\n",
      "Train Epoch: 11 [15360/30000 (51%)]\tLoss: 956.961060\n",
      "Train Epoch: 11 [17920/30000 (59%)]\tLoss: 990.459412\n",
      "Train Epoch: 11 [20480/30000 (68%)]\tLoss: 942.647339\n",
      "Train Epoch: 11 [23040/30000 (76%)]\tLoss: 953.321960\n",
      "Train Epoch: 11 [25600/30000 (85%)]\tLoss: 1000.594849\n",
      "Train Epoch: 11 [28160/30000 (93%)]\tLoss: 949.761841\n",
      "====> Epoch: 11 Average loss: 985.1898\n",
      "====> Test set loss: 929.1525\n",
      "Train Epoch: 12 [0/30000 (0%)]\tLoss: 970.091431\n",
      "Train Epoch: 12 [2560/30000 (8%)]\tLoss: 943.947510\n",
      "Train Epoch: 12 [5120/30000 (17%)]\tLoss: 963.352234\n",
      "Train Epoch: 12 [7680/30000 (25%)]\tLoss: 975.334656\n",
      "Train Epoch: 12 [10240/30000 (34%)]\tLoss: 960.133057\n",
      "Train Epoch: 12 [12800/30000 (42%)]\tLoss: 970.140259\n",
      "Train Epoch: 12 [15360/30000 (51%)]\tLoss: 970.638550\n",
      "Train Epoch: 12 [17920/30000 (59%)]\tLoss: 944.097229\n",
      "Train Epoch: 12 [20480/30000 (68%)]\tLoss: 980.972046\n",
      "Train Epoch: 12 [23040/30000 (76%)]\tLoss: 996.306641\n",
      "Train Epoch: 12 [25600/30000 (85%)]\tLoss: 958.214722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [28160/30000 (93%)]\tLoss: 967.076416\n",
      "====> Epoch: 12 Average loss: 963.4980\n",
      "====> Test set loss: 925.9156\n",
      "Train Epoch: 13 [0/30000 (0%)]\tLoss: 926.350952\n",
      "Train Epoch: 13 [2560/30000 (8%)]\tLoss: 928.695679\n",
      "Train Epoch: 13 [5120/30000 (17%)]\tLoss: 955.798828\n",
      "Train Epoch: 13 [7680/30000 (25%)]\tLoss: 927.883911\n",
      "Train Epoch: 13 [10240/30000 (34%)]\tLoss: 955.237915\n",
      "Train Epoch: 13 [12800/30000 (42%)]\tLoss: 990.962097\n",
      "Train Epoch: 13 [15360/30000 (51%)]\tLoss: 955.455261\n",
      "Train Epoch: 13 [17920/30000 (59%)]\tLoss: 971.458374\n",
      "Train Epoch: 13 [20480/30000 (68%)]\tLoss: 960.570740\n",
      "Train Epoch: 13 [23040/30000 (76%)]\tLoss: 952.443176\n",
      "Train Epoch: 13 [25600/30000 (85%)]\tLoss: 960.525208\n",
      "Train Epoch: 13 [28160/30000 (93%)]\tLoss: 980.232117\n",
      "====> Epoch: 13 Average loss: 959.1644\n",
      "====> Test set loss: 949.8974\n",
      "Train Epoch: 14 [0/30000 (0%)]\tLoss: 965.279053\n",
      "Train Epoch: 14 [2560/30000 (8%)]\tLoss: 954.932007\n",
      "Train Epoch: 14 [5120/30000 (17%)]\tLoss: 952.600830\n",
      "Train Epoch: 14 [7680/30000 (25%)]\tLoss: 929.013611\n",
      "Train Epoch: 14 [10240/30000 (34%)]\tLoss: 966.685547\n",
      "Train Epoch: 14 [12800/30000 (42%)]\tLoss: 955.188110\n",
      "Train Epoch: 14 [15360/30000 (51%)]\tLoss: 972.349121\n",
      "Train Epoch: 14 [17920/30000 (59%)]\tLoss: 918.754456\n",
      "Train Epoch: 14 [20480/30000 (68%)]\tLoss: 966.898560\n",
      "Train Epoch: 14 [23040/30000 (76%)]\tLoss: 991.812744\n",
      "Train Epoch: 14 [25600/30000 (85%)]\tLoss: 921.280273\n",
      "Train Epoch: 14 [28160/30000 (93%)]\tLoss: 960.419983\n",
      "====> Epoch: 14 Average loss: 959.5536\n",
      "====> Test set loss: 921.9436\n",
      "Train Epoch: 15 [0/30000 (0%)]\tLoss: 968.044556\n",
      "Train Epoch: 15 [2560/30000 (8%)]\tLoss: 959.206421\n",
      "Train Epoch: 15 [5120/30000 (17%)]\tLoss: 932.950012\n",
      "Train Epoch: 15 [7680/30000 (25%)]\tLoss: 1003.720093\n",
      "Train Epoch: 15 [10240/30000 (34%)]\tLoss: 992.452698\n",
      "Train Epoch: 15 [12800/30000 (42%)]\tLoss: 936.258728\n",
      "Train Epoch: 15 [15360/30000 (51%)]\tLoss: 923.885864\n",
      "Train Epoch: 15 [17920/30000 (59%)]\tLoss: 951.917603\n",
      "Train Epoch: 15 [20480/30000 (68%)]\tLoss: 1012.475037\n",
      "Train Epoch: 15 [23040/30000 (76%)]\tLoss: 970.294434\n",
      "Train Epoch: 15 [25600/30000 (85%)]\tLoss: 928.369751\n",
      "Train Epoch: 15 [28160/30000 (93%)]\tLoss: 913.025024\n",
      "====> Epoch: 15 Average loss: 953.3662\n",
      "====> Test set loss: 916.1668\n",
      "Train Epoch: 16 [0/30000 (0%)]\tLoss: 969.479614\n",
      "Train Epoch: 16 [2560/30000 (8%)]\tLoss: 975.263184\n",
      "Train Epoch: 16 [5120/30000 (17%)]\tLoss: 949.814331\n",
      "Train Epoch: 16 [7680/30000 (25%)]\tLoss: 953.172363\n",
      "Train Epoch: 16 [10240/30000 (34%)]\tLoss: 931.384644\n",
      "Train Epoch: 16 [12800/30000 (42%)]\tLoss: 979.902100\n",
      "Train Epoch: 16 [15360/30000 (51%)]\tLoss: 939.918579\n",
      "Train Epoch: 16 [17920/30000 (59%)]\tLoss: 947.993103\n",
      "Train Epoch: 16 [20480/30000 (68%)]\tLoss: 975.031860\n",
      "Train Epoch: 16 [23040/30000 (76%)]\tLoss: 929.736084\n",
      "Train Epoch: 16 [25600/30000 (85%)]\tLoss: 910.771912\n",
      "Train Epoch: 16 [28160/30000 (93%)]\tLoss: 933.248718\n",
      "====> Epoch: 16 Average loss: 949.8142\n",
      "====> Test set loss: 913.3538\n",
      "Train Epoch: 17 [0/30000 (0%)]\tLoss: 969.976196\n",
      "Train Epoch: 17 [2560/30000 (8%)]\tLoss: 953.858826\n",
      "Train Epoch: 17 [5120/30000 (17%)]\tLoss: 1003.145996\n",
      "Train Epoch: 17 [7680/30000 (25%)]\tLoss: 929.403931\n",
      "Train Epoch: 17 [10240/30000 (34%)]\tLoss: 962.895142\n",
      "Train Epoch: 17 [12800/30000 (42%)]\tLoss: 933.717407\n",
      "Train Epoch: 17 [15360/30000 (51%)]\tLoss: 981.094971\n",
      "Train Epoch: 17 [17920/30000 (59%)]\tLoss: 925.061890\n",
      "Train Epoch: 17 [20480/30000 (68%)]\tLoss: 985.301758\n",
      "Train Epoch: 17 [23040/30000 (76%)]\tLoss: 983.264221\n",
      "Train Epoch: 17 [25600/30000 (85%)]\tLoss: 936.129028\n",
      "Train Epoch: 17 [28160/30000 (93%)]\tLoss: 937.614868\n",
      "====> Epoch: 17 Average loss: 949.5573\n",
      "====> Test set loss: 933.2652\n",
      "Train Epoch: 18 [0/30000 (0%)]\tLoss: 961.728577\n",
      "Train Epoch: 18 [2560/30000 (8%)]\tLoss: 964.466309\n",
      "Train Epoch: 18 [5120/30000 (17%)]\tLoss: 976.643372\n",
      "Train Epoch: 18 [7680/30000 (25%)]\tLoss: 937.100891\n",
      "Train Epoch: 18 [10240/30000 (34%)]\tLoss: 949.999451\n",
      "Train Epoch: 18 [12800/30000 (42%)]\tLoss: 960.134521\n",
      "Train Epoch: 18 [15360/30000 (51%)]\tLoss: 967.398560\n",
      "Train Epoch: 18 [17920/30000 (59%)]\tLoss: 947.406799\n",
      "Train Epoch: 18 [20480/30000 (68%)]\tLoss: 929.480469\n",
      "Train Epoch: 18 [23040/30000 (76%)]\tLoss: 979.116150\n",
      "Train Epoch: 18 [25600/30000 (85%)]\tLoss: 955.568787\n",
      "Train Epoch: 18 [28160/30000 (93%)]\tLoss: 926.279175\n",
      "====> Epoch: 18 Average loss: 949.5669\n",
      "====> Test set loss: 912.1306\n",
      "Train Epoch: 19 [0/30000 (0%)]\tLoss: 960.033813\n",
      "Train Epoch: 19 [2560/30000 (8%)]\tLoss: 967.566711\n",
      "Train Epoch: 19 [5120/30000 (17%)]\tLoss: 951.001587\n",
      "Train Epoch: 19 [7680/30000 (25%)]\tLoss: 933.755432\n",
      "Train Epoch: 19 [10240/30000 (34%)]\tLoss: 953.323547\n",
      "Train Epoch: 19 [12800/30000 (42%)]\tLoss: 967.289551\n",
      "Train Epoch: 19 [15360/30000 (51%)]\tLoss: 941.848999\n",
      "Train Epoch: 19 [17920/30000 (59%)]\tLoss: 948.831299\n",
      "Train Epoch: 19 [20480/30000 (68%)]\tLoss: 983.518494\n",
      "Train Epoch: 19 [23040/30000 (76%)]\tLoss: 956.120239\n",
      "Train Epoch: 19 [25600/30000 (85%)]\tLoss: 915.154541\n",
      "Train Epoch: 19 [28160/30000 (93%)]\tLoss: 951.195374\n",
      "====> Epoch: 19 Average loss: 945.0487\n",
      "====> Test set loss: 914.0659\n",
      "Train Epoch: 20 [0/30000 (0%)]\tLoss: 943.565430\n",
      "Train Epoch: 20 [2560/30000 (8%)]\tLoss: 975.440979\n",
      "Train Epoch: 20 [5120/30000 (17%)]\tLoss: 961.452148\n",
      "Train Epoch: 20 [7680/30000 (25%)]\tLoss: 946.134338\n",
      "Train Epoch: 20 [10240/30000 (34%)]\tLoss: 978.714294\n",
      "Train Epoch: 20 [12800/30000 (42%)]\tLoss: 928.531616\n",
      "Train Epoch: 20 [15360/30000 (51%)]\tLoss: 914.165833\n",
      "Train Epoch: 20 [17920/30000 (59%)]\tLoss: 918.524780\n",
      "Train Epoch: 20 [20480/30000 (68%)]\tLoss: 945.828979\n",
      "Train Epoch: 20 [23040/30000 (76%)]\tLoss: 964.570862\n",
      "Train Epoch: 20 [25600/30000 (85%)]\tLoss: 887.042969\n",
      "Train Epoch: 20 [28160/30000 (93%)]\tLoss: 935.163879\n",
      "====> Epoch: 20 Average loss: 942.0886\n",
      "====> Test set loss: 905.8416\n",
      "Train Epoch: 21 [0/30000 (0%)]\tLoss: 975.781982\n",
      "Train Epoch: 21 [2560/30000 (8%)]\tLoss: 974.926514\n",
      "Train Epoch: 21 [5120/30000 (17%)]\tLoss: 964.023682\n",
      "Train Epoch: 21 [7680/30000 (25%)]\tLoss: 901.697510\n",
      "Train Epoch: 21 [10240/30000 (34%)]\tLoss: 943.224426\n",
      "Train Epoch: 21 [12800/30000 (42%)]\tLoss: 932.584229\n",
      "Train Epoch: 21 [15360/30000 (51%)]\tLoss: 916.894897\n",
      "Train Epoch: 21 [17920/30000 (59%)]\tLoss: 934.948120\n",
      "Train Epoch: 21 [20480/30000 (68%)]\tLoss: 958.112305\n",
      "Train Epoch: 21 [23040/30000 (76%)]\tLoss: 944.783813\n",
      "Train Epoch: 21 [25600/30000 (85%)]\tLoss: 994.853394\n",
      "Train Epoch: 21 [28160/30000 (93%)]\tLoss: 962.395386\n",
      "====> Epoch: 21 Average loss: 942.7776\n",
      "====> Test set loss: 912.4349\n",
      "Train Epoch: 22 [0/30000 (0%)]\tLoss: 936.892029\n",
      "Train Epoch: 22 [2560/30000 (8%)]\tLoss: 975.317871\n",
      "Train Epoch: 22 [5120/30000 (17%)]\tLoss: 902.799866\n",
      "Train Epoch: 22 [7680/30000 (25%)]\tLoss: 993.912659\n",
      "Train Epoch: 22 [10240/30000 (34%)]\tLoss: 934.309631\n",
      "Train Epoch: 22 [12800/30000 (42%)]\tLoss: 935.510925\n",
      "Train Epoch: 22 [15360/30000 (51%)]\tLoss: 955.844238\n",
      "Train Epoch: 22 [17920/30000 (59%)]\tLoss: 926.378662\n",
      "Train Epoch: 22 [20480/30000 (68%)]\tLoss: 960.853394\n",
      "Train Epoch: 22 [23040/30000 (76%)]\tLoss: 948.864624\n",
      "Train Epoch: 22 [25600/30000 (85%)]\tLoss: 932.339172\n",
      "Train Epoch: 22 [28160/30000 (93%)]\tLoss: 910.638428\n",
      "====> Epoch: 22 Average loss: 939.5463\n",
      "====> Test set loss: 908.7790\n",
      "Train Epoch: 23 [0/30000 (0%)]\tLoss: 961.622253\n",
      "Train Epoch: 23 [2560/30000 (8%)]\tLoss: 966.747314\n",
      "Train Epoch: 23 [5120/30000 (17%)]\tLoss: 925.623047\n",
      "Train Epoch: 23 [7680/30000 (25%)]\tLoss: 918.544739\n",
      "Train Epoch: 23 [10240/30000 (34%)]\tLoss: 944.178284\n",
      "Train Epoch: 23 [12800/30000 (42%)]\tLoss: 910.293274\n",
      "Train Epoch: 23 [15360/30000 (51%)]\tLoss: 966.132996\n",
      "Train Epoch: 23 [17920/30000 (59%)]\tLoss: 933.606750\n",
      "Train Epoch: 23 [20480/30000 (68%)]\tLoss: 955.289856\n",
      "Train Epoch: 23 [23040/30000 (76%)]\tLoss: 921.379395\n",
      "Train Epoch: 23 [25600/30000 (85%)]\tLoss: 930.618591\n",
      "Train Epoch: 23 [28160/30000 (93%)]\tLoss: 885.903748\n",
      "====> Epoch: 23 Average loss: 938.7015\n",
      "====> Test set loss: 908.1101\n",
      "Train Epoch: 24 [0/30000 (0%)]\tLoss: 973.123779\n",
      "Train Epoch: 24 [2560/30000 (8%)]\tLoss: 960.030945\n",
      "Train Epoch: 24 [5120/30000 (17%)]\tLoss: 941.423157\n",
      "Train Epoch: 24 [7680/30000 (25%)]\tLoss: 928.134277\n",
      "Train Epoch: 24 [10240/30000 (34%)]\tLoss: 925.009766\n",
      "Train Epoch: 24 [12800/30000 (42%)]\tLoss: 920.272766\n",
      "Train Epoch: 24 [15360/30000 (51%)]\tLoss: 955.351379\n",
      "Train Epoch: 24 [17920/30000 (59%)]\tLoss: 923.521790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [20480/30000 (68%)]\tLoss: 952.643311\n",
      "Train Epoch: 24 [23040/30000 (76%)]\tLoss: 932.950623\n",
      "Train Epoch: 24 [25600/30000 (85%)]\tLoss: 910.370605\n",
      "Train Epoch: 24 [28160/30000 (93%)]\tLoss: 959.138000\n",
      "====> Epoch: 24 Average loss: 940.2149\n",
      "====> Test set loss: 908.9862\n",
      "Train Epoch: 25 [0/30000 (0%)]\tLoss: 932.388489\n",
      "Train Epoch: 25 [2560/30000 (8%)]\tLoss: 960.806946\n",
      "Train Epoch: 25 [5120/30000 (17%)]\tLoss: 901.393555\n",
      "Train Epoch: 25 [7680/30000 (25%)]\tLoss: 900.159912\n",
      "Train Epoch: 25 [10240/30000 (34%)]\tLoss: 938.698914\n",
      "Train Epoch: 25 [12800/30000 (42%)]\tLoss: 953.331055\n",
      "Train Epoch: 25 [15360/30000 (51%)]\tLoss: 894.448853\n",
      "Train Epoch: 25 [17920/30000 (59%)]\tLoss: 914.876831\n",
      "Train Epoch: 25 [20480/30000 (68%)]\tLoss: 914.775208\n",
      "Train Epoch: 25 [23040/30000 (76%)]\tLoss: 972.104492\n",
      "Train Epoch: 25 [25600/30000 (85%)]\tLoss: 901.245667\n",
      "Train Epoch: 25 [28160/30000 (93%)]\tLoss: 930.224487\n",
      "====> Epoch: 25 Average loss: 937.7682\n",
      "====> Test set loss: 910.2454\n",
      "Train Epoch: 26 [0/30000 (0%)]\tLoss: 931.425659\n",
      "Train Epoch: 26 [2560/30000 (8%)]\tLoss: 955.631775\n",
      "Train Epoch: 26 [5120/30000 (17%)]\tLoss: 965.899780\n",
      "Train Epoch: 26 [7680/30000 (25%)]\tLoss: 956.750732\n",
      "Train Epoch: 26 [10240/30000 (34%)]\tLoss: 940.273682\n",
      "Train Epoch: 26 [12800/30000 (42%)]\tLoss: 913.256287\n",
      "Train Epoch: 26 [15360/30000 (51%)]\tLoss: 968.194763\n",
      "Train Epoch: 26 [17920/30000 (59%)]\tLoss: 922.635132\n",
      "Train Epoch: 26 [20480/30000 (68%)]\tLoss: 940.202515\n",
      "Train Epoch: 26 [23040/30000 (76%)]\tLoss: 944.914062\n",
      "Train Epoch: 26 [25600/30000 (85%)]\tLoss: 929.009766\n",
      "Train Epoch: 26 [28160/30000 (93%)]\tLoss: 937.310913\n",
      "====> Epoch: 26 Average loss: 936.3455\n",
      "====> Test set loss: 919.3252\n",
      "Train Epoch: 27 [0/30000 (0%)]\tLoss: 940.114868\n",
      "Train Epoch: 27 [2560/30000 (8%)]\tLoss: 957.631104\n",
      "Train Epoch: 27 [5120/30000 (17%)]\tLoss: 925.652832\n",
      "Train Epoch: 27 [7680/30000 (25%)]\tLoss: 966.553589\n",
      "Train Epoch: 27 [10240/30000 (34%)]\tLoss: 908.589417\n",
      "Train Epoch: 27 [12800/30000 (42%)]\tLoss: 945.326111\n",
      "Train Epoch: 27 [15360/30000 (51%)]\tLoss: 950.002136\n",
      "Train Epoch: 27 [17920/30000 (59%)]\tLoss: 973.769653\n",
      "Train Epoch: 27 [20480/30000 (68%)]\tLoss: 964.885193\n",
      "Train Epoch: 27 [23040/30000 (76%)]\tLoss: 942.789490\n",
      "Train Epoch: 27 [25600/30000 (85%)]\tLoss: 925.219788\n",
      "Train Epoch: 27 [28160/30000 (93%)]\tLoss: 921.716736\n",
      "====> Epoch: 27 Average loss: 937.1393\n",
      "====> Test set loss: 907.8209\n",
      "Train Epoch: 28 [0/30000 (0%)]\tLoss: 942.916382\n",
      "Train Epoch: 28 [2560/30000 (8%)]\tLoss: 875.661682\n",
      "Train Epoch: 28 [5120/30000 (17%)]\tLoss: 927.456665\n",
      "Train Epoch: 28 [7680/30000 (25%)]\tLoss: 929.360840\n",
      "Train Epoch: 28 [10240/30000 (34%)]\tLoss: 932.678284\n",
      "Train Epoch: 28 [12800/30000 (42%)]\tLoss: 885.801147\n",
      "Train Epoch: 28 [15360/30000 (51%)]\tLoss: 950.277649\n",
      "Train Epoch: 28 [17920/30000 (59%)]\tLoss: 926.988220\n",
      "Train Epoch: 28 [20480/30000 (68%)]\tLoss: 953.773987\n",
      "Train Epoch: 28 [23040/30000 (76%)]\tLoss: 910.238525\n",
      "Train Epoch: 28 [25600/30000 (85%)]\tLoss: 937.040405\n",
      "Train Epoch: 28 [28160/30000 (93%)]\tLoss: 936.190491\n",
      "====> Epoch: 28 Average loss: 935.4860\n",
      "====> Test set loss: 901.3303\n",
      "Train Epoch: 29 [0/30000 (0%)]\tLoss: 944.132385\n",
      "Train Epoch: 29 [2560/30000 (8%)]\tLoss: 915.181885\n",
      "Train Epoch: 29 [5120/30000 (17%)]\tLoss: 1001.469543\n",
      "Train Epoch: 29 [7680/30000 (25%)]\tLoss: 921.850220\n",
      "Train Epoch: 29 [10240/30000 (34%)]\tLoss: 939.502258\n",
      "Train Epoch: 29 [12800/30000 (42%)]\tLoss: 948.798096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-57:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 135, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 135, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 112, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/torch/functional.py\", line 66, in stack\n",
      "    return torch.cat(inputs, dim, out=out)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-40f8632f80da>\", line 5, in <module>\n",
      "    train(epoch)\n",
      "  File \"<ipython-input-4-da9e06842d3f>\", line 61, in train\n",
      "    for batch_idx, (data, _) in enumerate(train_loader):\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 275, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 254, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/queue.py\", line 164, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 293, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 718, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 372, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 406, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 161, in islink\n",
      "    st = os.lstat(path)\n",
      "  File \"/home/arijitx/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 135745) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "model=VAE().cuda(CUDA_DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "for epoch in range(1, 50+ 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    sample = Variable(torch.randn(64, 50)).cuda(CUDA_DEVICE)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    save_image(sample.data.view(64, 3, 150, 150),\n",
    "'results/sample_' + str(epoch) + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
